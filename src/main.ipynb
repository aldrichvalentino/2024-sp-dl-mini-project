{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet for CIFAR-10\n",
    "\n",
    "We take the inspiration for this code from the github link `pytorch-cifar`. It provides the basic block implementation of a Residual Network architecture. In our project, we declare a ResNet12 architecture, with four layers and `[2, 1, 1, 1]` blocks in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, dropout_rate=0.25):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Add dropout\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)  # Apply dropout after first activation\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, dropout_rate=0.5):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, dropout_rate=dropout_rate)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, dropout_rate=dropout_rate)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, dropout_rate=dropout_rate)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, dropout_rate=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Add dropout before the final layer\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride, dropout_rate):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, dropout_rate))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out)  # Apply dropout before the final fully connected layer\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet12():\n",
    "    return ResNet(BasicBlock, [2, 1, 1, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a few helper functions to import data and format training progress. Some are also taken from the `python-cifar` utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import zipfile\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "term_width = 50\n",
    "last_time = time.time()\n",
    "begin_time = last_time\n",
    "\n",
    "def progress_bar(current, total, msg=None):\n",
    "    bar_length = 60\n",
    "    progress = current / total\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    # Use '>' as an arrow to indicate current progress position\n",
    "    # Note: The arrow is positioned at the end of the filled portion, except when progress is 0\n",
    "    arrow = \">\" if block < bar_length else \"\"\n",
    "    text = \"\\rProgress: [{0}{1}{2}] {3:.2f}% ({4}/{5}) {6}\".format(\n",
    "        \"=\" * (block - 1 if block > 0 else 0), arrow,\n",
    "        \"-\" * (bar_length - block), progress * 100, current, total,\n",
    "        msg if msg else \"\")\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()\n",
    "    if current == total:\n",
    "        sys.stdout.write('\\n')\n",
    "\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    secondsf = int(seconds)\n",
    "    seconds = seconds - secondsf\n",
    "    millis = int(seconds*1000)\n",
    "\n",
    "    f = ''\n",
    "    i = 1\n",
    "    if days > 0:\n",
    "        f += str(days) + 'D'\n",
    "        i += 1\n",
    "    if hours > 0 and i <= 2:\n",
    "        f += str(hours) + 'h'\n",
    "        i += 1\n",
    "    if minutes > 0 and i <= 2:\n",
    "        f += str(minutes) + 'm'\n",
    "        i += 1\n",
    "    if secondsf > 0 and i <= 2:\n",
    "        f += str(secondsf) + 's'\n",
    "        i += 1\n",
    "    if millis > 0 and i <= 2:\n",
    "        f += str(millis) + 'ms'\n",
    "        i += 1\n",
    "    if f == '':\n",
    "        f = '0ms'\n",
    "    return f\n",
    "\n",
    "\n",
    "def unpickle(filename):\n",
    "    zf = zipfile.ZipFile(filename, \"r\")\n",
    "    data = pickle.loads(zf.open(\"cifar_test_nolabels.pkl\").read())\n",
    "    zf.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we instanciate our model and count the number of parameters. We also import the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Count params:  4977226\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "combined_set = torch.utils.data.ConcatDataset([trainset, testset])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    combined_set, batch_size=128, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "net = ResNet12()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(net)\n",
    "count_params = count_parameters(net)\n",
    "print(\"Count params: \", count_params)\n",
    "assert count_params < 5000000, \"Parameters must be lower than 5 million\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "For training, we use `CrossEntropyLoss` to meausre how the model is doing. This is a suitable loss for an image classification task. Next, we use the `Adam` optimizer to train the parameters. The learning rate will be updated using cosine annealing schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Training Optimizer\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx+1, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx+1, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                          % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our best result, we trained the model using 100 epochs with the RTX8000 GPU is NYU's HPC cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 1.331 | Acc: 51.388% (30833/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 1.163 | Acc: 58.800% (5880/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.886 | Acc: 68.675% (41205/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.692 | Acc: 75.300% (7530/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.681 | Acc: 76.240% (45744/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.633 | Acc: 78.580% (7858/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.559 | Acc: 80.590% (48354/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.462 | Acc: 83.560% (8356/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.478 | Acc: 83.638% (50183/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.364 | Acc: 87.860% (8786/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.419 | Acc: 85.438% (51263/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.403 | Acc: 85.630% (8563/10000)\n",
      "\n",
      "Epoch: 6\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.370 | Acc: 87.263% (52358/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.198 | Acc: 93.110% (9311/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.336 | Acc: 88.410% (53046/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.155 | Acc: 94.920% (9492/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.303 | Acc: 89.497% (53698/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.148 | Acc: 95.120% (9512/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.278 | Acc: 90.342% (54205/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.123 | Acc: 95.970% (9597/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 10\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.250 | Acc: 91.485% (54891/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.121 | Acc: 95.580% (9558/10000)\n",
      "\n",
      "Epoch: 11\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.238 | Acc: 91.743% (55046/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.097 | Acc: 97.090% (9709/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 12\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.218 | Acc: 92.487% (55492/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.103 | Acc: 96.310% (9631/10000)\n",
      "\n",
      "Epoch: 13\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.197 | Acc: 93.090% (55854/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.083 | Acc: 97.390% (9739/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 14\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.184 | Acc: 93.497% (56098/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.057 | Acc: 98.240% (9824/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 15\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.172 | Acc: 93.940% (56364/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.046 | Acc: 98.770% (9877/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 16\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.162 | Acc: 94.273% (56564/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.039 | Acc: 99.050% (9905/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 17\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.150 | Acc: 94.660% (56796/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.033 | Acc: 99.170% (9917/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 18\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.143 | Acc: 94.907% (56944/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.046 | Acc: 98.500% (9850/10000)\n",
      "\n",
      "Epoch: 19\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.132 | Acc: 95.295% (57177/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.030 | Acc: 99.180% (9918/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 20\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.124 | Acc: 95.650% (57390/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.038 | Acc: 98.870% (9887/10000)\n",
      "\n",
      "Epoch: 21\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.116 | Acc: 96.010% (57606/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.024 | Acc: 99.380% (9938/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 22\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.106 | Acc: 96.272% (57763/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.028 | Acc: 99.090% (9909/10000)\n",
      "\n",
      "Epoch: 23\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.105 | Acc: 96.357% (57814/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.022 | Acc: 99.320% (9932/10000)\n",
      "\n",
      "Epoch: 24\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.096 | Acc: 96.567% (57940/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.024 | Acc: 99.240% (9924/10000)\n",
      "\n",
      "Epoch: 25\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.091 | Acc: 96.698% (58019/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.033 | Acc: 98.910% (9891/10000)\n",
      "\n",
      "Epoch: 26\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.087 | Acc: 96.968% (58181/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.027 | Acc: 99.180% (9918/10000)\n",
      "\n",
      "Epoch: 27\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.082 | Acc: 97.112% (58267/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.018 | Acc: 99.510% (9951/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 28\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.075 | Acc: 97.320% (58392/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.031 | Acc: 98.820% (9882/10000)\n",
      "\n",
      "Epoch: 29\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.077 | Acc: 97.202% (58321/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.022 | Acc: 99.220% (9922/10000)\n",
      "\n",
      "Epoch: 30\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.071 | Acc: 97.517% (58510/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.016 | Acc: 99.430% (9943/10000)\n",
      "\n",
      "Epoch: 31\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.068 | Acc: 97.633% (58580/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.014 | Acc: 99.540% (9954/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 32\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.065 | Acc: 97.740% (58644/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.015 | Acc: 99.500% (9950/10000)\n",
      "\n",
      "Epoch: 33\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.060 | Acc: 97.905% (58743/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.010 | Acc: 99.660% (9966/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 34\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.058 | Acc: 97.988% (58793/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.015 | Acc: 99.490% (9949/10000)\n",
      "\n",
      "Epoch: 35\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.059 | Acc: 97.892% (58735/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.010 | Acc: 99.710% (9971/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 36\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.055 | Acc: 98.103% (58862/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.009 | Acc: 99.740% (9974/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 37\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.048 | Acc: 98.308% (58985/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.006 | Acc: 99.850% (9985/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 38\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.048 | Acc: 98.303% (58982/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.014 | Acc: 99.540% (9954/10000)\n",
      "\n",
      "Epoch: 39\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.048 | Acc: 98.290% (58974/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.013 | Acc: 99.570% (9957/10000)\n",
      "\n",
      "Epoch: 40\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.048 | Acc: 98.230% (58938/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.006 | Acc: 99.900% (9990/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 41\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.042 | Acc: 98.552% (59131/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.011 | Acc: 99.650% (9965/10000)\n",
      "\n",
      "Epoch: 42\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.043 | Acc: 98.508% (59105/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.009 | Acc: 99.690% (9969/10000)\n",
      "\n",
      "Epoch: 43\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.042 | Acc: 98.527% (59116/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.010 | Acc: 99.670% (9967/10000)\n",
      "\n",
      "Epoch: 44\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.040 | Acc: 98.585% (59151/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.007 | Acc: 99.760% (9976/10000)\n",
      "\n",
      "Epoch: 45\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.036 | Acc: 98.823% (59294/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.009 | Acc: 99.700% (9970/10000)\n",
      "\n",
      "Epoch: 46\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.041 | Acc: 98.602% (59161/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.007 | Acc: 99.830% (9983/10000)\n",
      "\n",
      "Epoch: 47\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.034 | Acc: 98.807% (59284/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.012 | Acc: 99.580% (9958/10000)\n",
      "\n",
      "Epoch: 48\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.034 | Acc: 98.818% (59291/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.006 | Acc: 99.800% (9980/10000)\n",
      "\n",
      "Epoch: 49\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.032 | Acc: 98.898% (59339/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.005 | Acc: 99.860% (9986/10000)\n",
      "\n",
      "Epoch: 50\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.031 | Acc: 98.940% (59364/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.007 | Acc: 99.730% (9973/10000)\n",
      "\n",
      "Epoch: 51\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.033 | Acc: 98.872% (59323/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.005 | Acc: 99.810% (9981/10000)\n",
      "\n",
      "Epoch: 52\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.031 | Acc: 98.940% (59364/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.005 | Acc: 99.840% (9984/10000)\n",
      "\n",
      "Epoch: 53\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.030 | Acc: 98.905% (59343/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.005 | Acc: 99.860% (9986/10000)\n",
      "\n",
      "Epoch: 54\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.030 | Acc: 98.987% (59392/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.007 | Acc: 99.710% (9971/10000)\n",
      "\n",
      "Epoch: 55\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.029 | Acc: 98.955% (59373/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.005 | Acc: 99.890% (9989/10000)\n",
      "\n",
      "Epoch: 56\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.027 | Acc: 99.102% (59461/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.007 | Acc: 99.820% (9982/10000)\n",
      "\n",
      "Epoch: 57\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.025 | Acc: 99.083% (59450/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.004 | Acc: 99.880% (9988/10000)\n",
      "\n",
      "Epoch: 58\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.021 | Acc: 99.263% (59558/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.003 | Acc: 99.940% (9994/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.026 | Acc: 99.043% (59426/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.004 | Acc: 99.890% (9989/10000)\n",
      "\n",
      "Epoch: 60\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.027 | Acc: 99.032% (59419/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.003 | Acc: 99.960% (9996/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 61\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.024 | Acc: 99.170% (59502/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.005 | Acc: 99.850% (9985/10000)\n",
      "\n",
      "Epoch: 62\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.021 | Acc: 99.267% (59560/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.004 | Acc: 99.870% (9987/10000)\n",
      "\n",
      "Epoch: 63\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.019 | Acc: 99.333% (59600/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.002 | Acc: 99.950% (9995/10000)\n",
      "\n",
      "Epoch: 64\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.021 | Acc: 99.285% (59571/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.003 | Acc: 99.910% (9991/10000)\n",
      "\n",
      "Epoch: 65\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.023 | Acc: 99.188% (59513/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.009 | Acc: 99.660% (9966/10000)\n",
      "\n",
      "Epoch: 66\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.018 | Acc: 99.362% (59617/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.003 | Acc: 99.900% (9990/10000)\n",
      "\n",
      "Epoch: 67\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.019 | Acc: 99.323% (59594/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.003 | Acc: 99.900% (9990/10000)\n",
      "\n",
      "Epoch: 68\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.019 | Acc: 99.340% (59604/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.002 | Acc: 99.870% (9987/10000)\n",
      "\n",
      "Epoch: 69\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.019 | Acc: 99.345% (59607/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.005 | Acc: 99.810% (9981/10000)\n",
      "\n",
      "Epoch: 70\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.018 | Acc: 99.350% (59610/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.005 | Acc: 99.870% (9987/10000)\n",
      "\n",
      "Epoch: 71\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.019 | Acc: 99.372% (59623/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.006 | Acc: 99.780% (9978/10000)\n",
      "\n",
      "Epoch: 72\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.018 | Acc: 99.385% (59631/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.004 | Acc: 99.890% (9989/10000)\n",
      "\n",
      "Epoch: 73\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.016 | Acc: 99.473% (59684/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.970% (9997/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 74\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.013 | Acc: 99.562% (59737/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.002 | Acc: 99.930% (9993/10000)\n",
      "\n",
      "Epoch: 75\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.015 | Acc: 99.488% (59693/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.002 | Acc: 99.960% (9996/10000)\n",
      "\n",
      "Epoch: 76\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.015 | Acc: 99.448% (59669/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.007 | Acc: 99.810% (9981/10000)\n",
      "\n",
      "Epoch: 77\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.016 | Acc: 99.430% (59658/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.004 | Acc: 99.870% (9987/10000)\n",
      "\n",
      "Epoch: 78\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.015 | Acc: 99.473% (59684/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.003 | Acc: 99.920% (9992/10000)\n",
      "\n",
      "Epoch: 79\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.012 | Acc: 99.567% (59740/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.980% (9998/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 80\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.012 | Acc: 99.563% (59738/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.970% (9997/10000)\n",
      "\n",
      "Epoch: 81\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.015 | Acc: 99.453% (59672/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.980% (9998/10000)\n",
      "\n",
      "Epoch: 82\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.010 | Acc: 99.642% (59785/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.002 | Acc: 99.960% (9996/10000)\n",
      "\n",
      "Epoch: 83\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.013 | Acc: 99.552% (59731/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.980% (9998/10000)\n",
      "\n",
      "Epoch: 84\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.012 | Acc: 99.598% (59759/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.950% (9995/10000)\n",
      "\n",
      "Epoch: 85\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.012 | Acc: 99.597% (59758/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.960% (9996/10000)\n",
      "\n",
      "Epoch: 86\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.013 | Acc: 99.578% (59747/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.002 | Acc: 99.940% (9994/10000)\n",
      "\n",
      "Epoch: 87\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.012 | Acc: 99.550% (59730/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.950% (9995/10000)\n",
      "\n",
      "Epoch: 88\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.009 | Acc: 99.673% (59804/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.970% (9997/10000)\n",
      "\n",
      "Epoch: 89\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.010 | Acc: 99.682% (59809/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.960% (9996/10000)\n",
      "\n",
      "Epoch: 90\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.012 | Acc: 99.597% (59758/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.980% (9998/10000)\n",
      "\n",
      "Epoch: 91\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.011 | Acc: 99.610% (59766/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.970% (9997/10000)\n",
      "\n",
      "Epoch: 92\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.008 | Acc: 99.740% (59844/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.950% (9995/10000)\n",
      "\n",
      "Epoch: 93\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.009 | Acc: 99.675% (59805/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.002 | Acc: 99.940% (9994/10000)\n",
      "\n",
      "Epoch: 94\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.011 | Acc: 99.640% (59784/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.940% (9994/10000)\n",
      "\n",
      "Epoch: 95\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.008 | Acc: 99.740% (59844/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.000 | Acc: 100.000% (10000/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 96\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.008 | Acc: 99.733% (59840/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.910% (9991/10000)\n",
      "\n",
      "Epoch: 97\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.007 | Acc: 99.743% (59846/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.000 | Acc: 100.000% (10000/10000)\n",
      "\n",
      "Epoch: 98\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.008 | Acc: 99.753% (59852/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.990% (9999/10000)\n",
      "\n",
      "Epoch: 99\n",
      "Progress: [===========================================================] 100.00% (469/469) Loss: 0.007 | Acc: 99.782% (59869/60000)\n",
      "Progress: [===========================================================] 100.00% (100/100) Loss: 0.001 | Acc: 99.990% (9999/10000)\n"
     ]
    }
   ],
   "source": [
    "# Do 100 epoch\n",
    "for epoch in range(start_epoch, start_epoch+500):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "For Kaggle's data prediction, we will import the trained model and start predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "net = ResNet12()\n",
    "net = net.to(device)\n",
    "net = torch.nn.DataParallel(net)\n",
    "if device == 'cuda':\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# load state\n",
    "saved_model = torch.load('./checkpoint/ckpt.pth', map_location=torch.device(device))\n",
    "net.load_state_dict(saved_model['net'])\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the raw data and predict in batches of a thousand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "cifar_test_nolabels = unpickle(\"../data/cifar_test_nolabels.pkl.zip\")\n",
    "competition_data = cifar_test_nolabels[b'data']\n",
    "y_id = cifar_test_nolabels[b'ids']\n",
    "\n",
    "competitionset = []\n",
    "for d in competition_data:\n",
    "    # transpose flatten CIFAR image to RGB\n",
    "    d = d.reshape(3, 32, 32).transpose(1,2,0)\n",
    "    im = Image.fromarray(d, mode='RGB')\n",
    "    im = transform_test(im)\n",
    "    competitionset.append(im)\n",
    "\n",
    "competitionloader = torch.utils.data.DataLoader(\n",
    "    competitionset, batch_size=1000, shuffle=False)\n",
    "\n",
    "labels = []\n",
    "\n",
    "def competition_test():\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, input_test in enumerate(competitionloader):\n",
    "            input_test = input_test.to(device)\n",
    "            outputs = net(input_test.float())\n",
    "            _, predicted = outputs.max(1)\n",
    "            labels.append(predicted)\n",
    "            print(len(predicted))\n",
    "\n",
    "            \n",
    "competition_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write the prediction labels into a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# flatten the labels\n",
    "pred = np.array([])\n",
    "for batch in labels:\n",
    "    pred = np.append(pred, batch.tolist())\n",
    "\n",
    "\n",
    "competition_data_ids = unpickle(\"../data/cifar_test_nolabels.pkl.zip\")[b'ids']\n",
    "    \n",
    "# create dataframe to save prediction\n",
    "df = pd.DataFrame()\n",
    "df['ID'] = competition_data_ids\n",
    "df['Label'] = pred\n",
    "df['Label'] = df['Label'].astype(int)\n",
    "\n",
    "df.to_csv('out.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
